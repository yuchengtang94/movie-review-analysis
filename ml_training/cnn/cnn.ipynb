{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "832879fbf0b90cb75b9b2571d6b9b8ebbd43ca7c"
   },
   "source": [
    "In this kernel we will use a simple Convolutional Neural Network to tackle the problem at hand. CNNs are fast and produce adequate enough results, so this will serve as a pretty good baseline for more sophisticated architectures revolving around CNNs.\n",
    "\n",
    "Before we begin, we will set the seed for the components involved. That is, plain ol' Python, Numpy and Tensorflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "75e37e5abb691df18a94631fc63429452b8e3650",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 0\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "set_random_seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "04ac3fb55c1d2f9ce60384b240fe1b5e445020e9"
   },
   "source": [
    "Now we will read our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv('../input/train.tsv',  sep=\"\\t\")\n",
    "test = pd.read_csv('../input/test.tsv',  sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0261c62c3b24f293de41db76dca8f78caea96c45",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "85548b537e6fd30cf53fef7edbb5dbac965c15ef"
   },
   "source": [
    "The mean and max lengths of phrases are the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "79c59246fcf9702a5b4b3dea42f88af5cbed8449",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Phrase'].str.len().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "44be519fad02c4b21187bfbf388c42cdead077ce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Phrase'].str.len().max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cfef5dcb6e16a7abd62cdacc3451100eba2428ed"
   },
   "source": [
    "Let's take a look at how sentiments are distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6770a64f795c74ae6808fbae1ad0ddc188181bda",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['Sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9e83f494ff76dfd424f7b4b8f089849a67a0d5cb"
   },
   "source": [
    "The values correspond to sentiments as follows:\n",
    "\n",
    "```\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive\n",
    "```\n",
    "\n",
    "We can see that most phrases are neutral, followed by the 'somewhats' (somewhat positive - somewhat negative). Then, far behind are positive and negative phrases. This (seemingly) makes classification harder, since most phrases are congregated towards the middle/neutral.\n",
    "\n",
    "To train our model, we need to format our data.\n",
    "\n",
    "Since we are dealing with text, we will first convert everything to lowercase. Then, we will tokenize our text. Currently, we build the tokenizer only on the training data. We could add to the ingredients the testing data, but results may go up or down. Testing is needed to determine whether or not adding the testing data will help. After the tokenization, we also need to pad the rows with zeros.\n",
    "\n",
    "Apart from that, we need to convert the numerical output to categorical. Specifically, we need to one-hot encode the labels.\n",
    "\n",
    "Finally, we need to shuffle our data as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "9de77d1b021b8b009941da1f5eba9271fc7b91d1",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_data(train, test, max_features, maxlen):\n",
    "    \"\"\"\n",
    "    Convert data to proper format.\n",
    "    1) Shuffle\n",
    "    2) Lowercase\n",
    "    3) Sentiments to Categorical\n",
    "    4) Tokenize and Fit\n",
    "    5) Convert to sequence (format accepted by the network)\n",
    "    6) Pad\n",
    "    7) Voila!\n",
    "    \"\"\"\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils import to_categorical\n",
    "    \n",
    "    train = train.sample(frac=1).reset_index(drop=True)\n",
    "    train['Phrase'] = train['Phrase'].apply(lambda x: x.lower())\n",
    "    test['Phrase'] = test['Phrase'].apply(lambda x: x.lower())\n",
    "\n",
    "    X = train['Phrase']\n",
    "    test_X = test['Phrase']\n",
    "    Y = to_categorical(train['Sentiment'].values)\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "    tokenizer.fit_on_texts(list(X))\n",
    "\n",
    "    X = tokenizer.texts_to_sequences(X)\n",
    "    X = pad_sequences(X, maxlen=maxlen)\n",
    "    test_X = tokenizer.texts_to_sequences(test_X)\n",
    "    test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "    return X, Y, test_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0cf2b30b50701c41c7da82b285df5edf597647cc",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 125\n",
    "max_features = 10000\n",
    "\n",
    "X, Y, test_X = format_data(train, test, max_features, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4388e6f9e996e63c6159433e11bac988dc9b88b7"
   },
   "source": [
    "Let's take a look at how the data looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f873b10be1a386de642e3f943609793ec3cda0e7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "539b23fa68dea5902ed0916c8c2a1944728bba75"
   },
   "source": [
    "As you can see each row is zero-padded on the left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "94f1985ffa34ea803dc9ea0aaca4add9c2aaae60",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0a99ce6d0ef50c977796c0643913da3d1736d60b",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7b3ac7da30805589b4c0f3820188f416d194d2ba"
   },
   "source": [
    "With the formatted data at hand, we move to split our training set to training and validation. The validation set will help as determine whether our model generalizes well or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "87a8e790a08d921810dc2fe5c38740a905e453d9",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e591a6510cedb656ebba84c4d474d701a7717d04"
   },
   "source": [
    "We will now start building our model.\n",
    "\n",
    "Since we are dealing with sequences (the tokens) the input should be an embedding layer. To avoid overfitting, we will use `SpatialDropout` to drop some of the neurons in the embedding.\n",
    "\n",
    "Afterwards, we build a CNN as normal. Since data is one-dimensional, we only need one-dimensional convolutions. After each convolution, we use `MaxPooling` to merge neighboring activations together. This will reduce the size of each layer and will keep the network from overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1c089a881fa66210724378a5927d3ab27945eb60",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Embedding, Flatten\n",
    "from keras.layers import SpatialDropout1D\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "866799ad61088c89baecb72ff2a0a37bde4a3eaf",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input / Embdedding\n",
    "model.add(Embedding(max_features, 150, input_length=maxlen))\n",
    "\n",
    "# CNN\n",
    "model.add(SpatialDropout1D(0.2))\n",
    "\n",
    "model.add(Conv1D(32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Conv1D(64, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Output layer\n",
    "model.add(Dense(5, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "78b5a66785119a37d4b67ce37e2b535475b41e07"
   },
   "source": [
    "Having build our network, we will start training.\n",
    "\n",
    "Since this problem is multi-class classification, we will optimize the categorical crossentropy loss function. The optimizer we will use is ADAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "eb5e9a193d224c189a7eccfdb39c20668226dce3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8c4aee226faf679bc6663d09df69e7dccd169a3a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=epochs, batch_size=batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "969bc9fde2967fdf6bdf3727761dc0a46c2ca7bc"
   },
   "source": [
    "Finally, we will make our predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d7a864588f97c857b121c54de85fe2a9e4c16a00",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub = pd.read_csv('../input/sampleSubmission.csv')\n",
    "\n",
    "sub['Sentiment'] = model.predict_classes(test_X, batch_size=batch_size, verbose=1)\n",
    "sub.to_csv('sub_cnn.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
